{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPT2_cYllIII"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from typing import Dict, Optional, List\n",
        "import os\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import queue\n",
        "import sys\n",
        "\n",
        "from collections import defaultdict\n",
        "from contextlib import nullcontext\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from typing import Union, List, Tuple, Any\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data._utils.worker import ManagerWatchdog\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModel, is_torch_npu_available\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26e92b5a"
      },
      "source": [
        "!pip install flash_attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Qwen3Reranker:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name_or_path: str,\n",
        "        max_length: int = 4096,\n",
        "        instruction=None,\n",
        "        attn_type='causal',\n",
        "    ) -> None:\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "        self.max_length=max_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, padding_side='left')\n",
        "        self.lm = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\n",
        "        self.token_false_id = self.tokenizer.convert_tokens_to_ids(\"no\")\n",
        "        self.token_true_id = self.tokenizer.convert_tokens_to_ids(\"yes\")\n",
        "\n",
        "        self.prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
        "        self.suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
        "\n",
        "        self.prefix_tokens = self.tokenizer.encode(self.prefix, add_special_tokens=False)\n",
        "        self.suffix_tokens = self.tokenizer.encode(self.suffix, add_special_tokens=False)\n",
        "        self.instruction = instruction\n",
        "        if self.instruction is None:\n",
        "            self.instruction = \"Given the user query, retrieval the relevant passages\"\n",
        "\n",
        "    def format_instruction(self, instruction, query, doc):\n",
        "        if instruction is None:\n",
        "            instruction = self.instruction\n",
        "        output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n",
        "        return output\n",
        "\n",
        "    def process_inputs(self, pairs):\n",
        "        out = self.tokenizer(\n",
        "            pairs, padding=False, truncation='longest_first',\n",
        "            return_attention_mask=False, max_length=self.max_length - len(self.prefix_tokens) - len(self.suffix_tokens)\n",
        "        )\n",
        "        for i, ele in enumerate(out['input_ids']):\n",
        "            out['input_ids'][i] = self.prefix_tokens + ele + self.suffix_tokens\n",
        "        out = self.tokenizer.pad(out, padding=True, return_tensors=\"pt\", max_length=self.max_length)\n",
        "        for key in out:\n",
        "            out[key] = out[key].to(self.lm.device)\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def compute_logits(self, inputs, **kwargs):\n",
        "\n",
        "        batch_scores = self.lm(**inputs).logits[:, -1, :]\n",
        "        true_vector = batch_scores[:, self.token_true_id]\n",
        "        false_vector = batch_scores[:, self.token_false_id]\n",
        "        batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
        "        batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
        "        scores = batch_scores[:, 1].exp().tolist()\n",
        "        return scores\n",
        "\n",
        "    def compute_scores(\n",
        "        self,\n",
        "        pairs,\n",
        "        instruction=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        pairs = [self.format_instruction(instruction, query, doc) for query, doc in pairs]\n",
        "        inputs = self.process_inputs(pairs)\n",
        "        scores = self.compute_logits(inputs)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "frU2wAudlOC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Qwen3Reranker(model_name_or_path='Qwen/Qwen3-Reranker-0.6B', instruction=\"Retrieval document that can answer user's query\", max_length=2048)\n"
      ],
      "metadata": {
        "id": "idBb7zBilN-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = ['Explain gravity','Explain gravity']\n",
        "documents = [\n",
        "    \"The capital of China is Beijing.\",\n",
        "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n",
        "]\n",
        "pairs = list(zip(queries, documents))\n",
        "instruction=\"Given the user query, retrieve the relevant passages\"\n"
      ],
      "metadata": {
        "id": "m3FdzzxflNys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The reranker takes the query + each candidate document and computes a relevance score, then sorts candidates accordingly.\n",
        "new_scores = model.compute_scores(pairs, instruction)\n",
        "print('scores', new_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAO16mT9lNmw",
        "outputId": "727baeee-e233-4f76-c3ad-533c85beea47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scores [2.753734588623047e-05, 0.9990234375]\n"
          ]
        }
      ]
    }
  ]
}